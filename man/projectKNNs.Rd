% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/prjectKNNsbackup.R, R/projectKNNs.R
\name{projectKNNs}
\alias{projectKNNs}
\title{Project a distance matrix into a lower-dimensional space}
\usage{
projectKNNs(x, dim, sgd.batches = nrow(x) * 10000, M = 5,
  weight.pos.samples = TRUE, distance.function = "a", gamma = 7,
  alpha = 2, rho = 1, min.rho = 0.1, verbose = TRUE)

projectKNNs(x, dim, sgd.batches = nrow(x) * 10000, M = 5,
  weight.pos.samples = TRUE, distance.function = "a", gamma = 7,
  alpha = 2, rho = 1, min.rho = 0.1, verbose = TRUE)
}
\arguments{
\item{x}{A sparse matrix in triplet form}

\item{dim}{The number of dimensions for the projection space}

\item{sgd.batches}{The number of edges to process during SGD; defaults to 10000 * the number of rows in x}

\item{M}{The number of negative edges to sample for each positive edge}

\item{weight.pos.samples}{Whether to sample positive edges according to their edge weights (the default) or multiply the edge-loss by the edge-weight in the objective function.}

\item{distance.function}{A function mapping the distance between two vertices in the lower-dimensional space to the probability that they are kNN's of each other.
\eqn{f(||y_1 - y_2||) = P(e_{ij} = 1)}}

\item{gamma}{Hyperparameter used in the objective function.  See the papers for details}

\item{alpha}{Hyperparameter used in distance function a}

\item{rho}{Initial learning rate.}

\item{min.rho}{Final learning rate. The learning rate declines non-linearly.  \eqn{\rho_t = \rho_{t-1} - ((\rho_{t-1} - \rho_{min}) / sgd.batches)}}

\item{verbose}{Verbosity}

\item{x}{A sparse matrix in triplet form}

\item{dim}{The number of dimensions for the projection space}

\item{sgd.batches}{The number of edges to process during SGD; defaults to 10000 * the number of rows in x}

\item{M}{The number of negative edges to sample for each positive edge}

\item{distance.function}{A function mapping the distance between two vertices in the lower-dimensional space to the probability that they are kNN's of each other.  (\eqn{f(||y_1 - y_2||) = P(e_{ij} = 1)}) The choices are a, \eqn{1 / (1 + \alpha * x^2)},
and b, \eqn{1 / (1 + exp(-x^2))}.}

\item{gamma}{Hyperparameter controlling the weight given to each negative sample.}

\item{weight.pos.samples}{Whether to sample positive edges according to their edge weights (the default) or multiply the edge-loss by the edge-weight in the objective function.}

\item{verbose}{Verbosity}
}
\value{
A dense [nrow(x),dim] matrix of the coordinates projecting x into the lower-dimensional space.

A dense [nrow(x),dim] matrix of the coordinates projecting x into the lower-dimensional space.
}
\description{
The input is a sparse triplet matrix showing the distances between vertices which are presumably estimated k-nearest-neighbors.
The algorithm attempts to estimate a \code{dim}-dimensional embedding using stochastic gradient descent.

The input is a sparse triplet matrix showing the distances between vertices which are presumably estimated k-nearest-neighbors.
The algorithm attempts to estimate a \code{dim}-dimensional embedding using stochastic gradient descent.
}
\details{
The objective function is: \deqn{ O = \sum_{(i,j)\in E} w_{ij} (\log p(e_{ij} = 1) + \sum_{k=1}^{M} E_{jk~P_{n}(j)} \gamma \log(1 - p(e_{ij_k} - 1)))  }

The objective function is: \deqn{ O = \sum_{(i,j)\in E} w_{ij} (\log p(e_{ij} = 1) + \sum_{k=1}^{M} E_{jk~P_{n}(j)} \gamma \log(1 - p(e_{ij_k} - 1)))  }
}

