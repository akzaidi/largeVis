% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/prjectKNNsbackup.R, R/prjectKNNsbackup2.R, R/projectKNNs.R
\name{projectKNNs}
\alias{projectKNNs}
\title{Project a distance matrix into a lower-dimensional space}
\usage{
projectKNNs(x, dim, sgd.batches = nrow(x) * 10000, M = 5,
  weight.pos.samples = TRUE, gamma = 7, alpha = 2, rho = 1,
  min.rho = 0.1, coords = NULL, verbose = TRUE)

projectKNNs(x, dim, sgd.batches = nrow(x) * 10000, M = 5,
  weight.pos.samples = TRUE, gamma = 7, alpha = 2, rho = 1,
  min.rho = 0.1, coords = NULL, verbose = TRUE)

projectKNNs(x, dim, sgd.batches = nrow(x) * 10000, M = 5,
  weight.pos.samples = TRUE, gamma = 7, alpha = 2, rho = 1,
  min.rho = 0.1, coords = NULL, verbose = TRUE)
}
\arguments{
\item{x}{A sparse matrix in triplet form}

\item{dim}{The number of dimensions for the projection space}

\item{sgd.batches}{The number of edges to process during SGD; defaults to 10000 * the number of rows in x}

\item{M}{The number of negative edges to sample for each positive edge}

\item{weight.pos.samples}{Whether to sample positive edges according to their edge weights (the default) or multiply the edge-loss by the edge-weight in the objective function.}

\item{gamma}{Hyperparameter used in the objective function.  See the papers for details}

\item{alpha}{Hyperparameter used in distance function a}

\item{rho}{Initial learning rate.}

\item{min.rho}{Final learning rate. The learning rate declines non-linearly.  \eqn{\rho_t = \rho_{t-1} - ((\rho_{t-1} - \rho_{min}) / sgd.batches)}}

\item{coords}{An initialized coordinate matrix.}

\item{verbose}{Verbosity}

\item{distance.function}{A function mapping the distance between two vertices in the lower-dimensional space to the probability that they are kNN's of each other.
\eqn{f(||y_1 - y_2||) = P(e_{ij} = 1)}}

\item{x}{A sparse matrix in triplet form}

\item{dim}{The number of dimensions for the projection space}

\item{sgd.batches}{The number of edges to process during SGD; defaults to 10000 * the number of rows in x}

\item{M}{The number of negative edges to sample for each positive edge}

\item{distance.function}{A function mapping the distance between two vertices in the lower-dimensional space to the probability that they are kNN's of each other.  (\eqn{f(||y_1 - y_2||) = P(e_{ij} = 1)}) The choices are a, \eqn{1 / (1 + \alpha * x^2)},
and b, \eqn{1 / (1 + exp(-x^2))}.}

\item{gamma}{Hyperparameter controlling the weight given to each negative sample.}

\item{control.outliers}{If a number, how often to trim outlying vertices.  The projection algorithm can easily produce outliers in its early iterations.
These tend to ultimately come under control as \eqn{\rho} decreases toward zero, however controlling the outliers manually may speed convergence.
If this parameter is given, periodically check to see if any vertices are more than 6 standard deviations from the mean of any dimension, and if so move them
two standard deviations toward the mean. The default is \code{FALSE}, consistent with the paper.}

\item{weight.pos.samples}{Whether to sample positive edges according to their edge weights (the default) or multiply the edge-loss by the edge-weight in the objective function.}

\item{tol}{Tolerance.  If a number, continuously compute the rolling average squared gradient of the last N batches, and terminate if it falls below \code{tol}. The default is \code{FALSE}.}

\item{verbose}{Verbosity}

\item{x}{A sparse matrix in triplet form}

\item{dim}{The number of dimensions for the projection space}

\item{sgd.batches}{The number of edges to process during SGD; defaults to 10000 * the number of rows in x}

\item{M}{The number of negative edges to sample for each positive edge}

\item{alpha}{Hyperparameter used in the default distance function, \eqn{1 / (1 + \alpha \dot ||y_i - y_j||^2)}.  If \code{alpha} is set to 0, the alternative distance
function \eqn{1 / 1 + exp(||y_i - y_j||^2)} is used instead.  These functions relate the distance between points in the low-dimensional projection to the likelihood
that they two points are nearest neighbors.}

\item{gamma}{Hyperparameter controlling the weight given to each negative sample.}

\item{weight.pos.samples}{Whether to sample positive edges according to their edge weights (the default) or multiply the edge-loss by the edge-weight in the objective function.}

\item{rho}{Initial learning rate.}

\item{min.rho}{Final learning rate. The learning rate declines non-linearly.  \eqn{\rho_t = \rho_{t-1} - ((\rho_{t-1} - \rho_{min}) / sgd.batches)}}

\item{coords}{An initialized coordinate matrix.}

\item{verbose}{Verbosity}
}
\value{
A dense [nrow(x),dim] matrix of the coordinates projecting x into the lower-dimensional space.

A dense [nrow(x),dim] matrix of the coordinates projecting x into the lower-dimensional space.

A dense [nrow(x),dim] matrix of the coordinates projecting x into the lower-dimensional space.
}
\description{
The input is a sparse triplet matrix showing the distances between vertices which are presumably estimated k-nearest-neighbors.
The algorithm attempts to estimate a \code{dim}-dimensional embedding using stochastic gradient descent.

The input is a sparse triplet matrix showing the distances between vertices which are presumably estimated k-nearest-neighbors.
The algorithm attempts to estimate a \code{dim}-dimensional embedding using stochastic gradient descent.

The input is a sparse triplet matrix showing the distances between vertices which are presumably estimated k-nearest-neighbors.
The algorithm attempts to estimate a \code{dim}-dimensional embedding using stochastic gradient descent.
}
\details{
The objective function is: \deqn{ O = \sum_{(i,j)\in E} w_{ij} (\log p(e_{ij} = 1) + \sum_{k=1}^{M} E_{jk~P_{n}(j)} \gamma \log(1 - p(e_{ij_k} - 1)))  }

The objective function is: \deqn{ O = \sum_{(i,j)\in E} w_{ij} (\log p(e_{ij} = 1) + \sum_{k=1}^{M} E_{jk~P_{n}(j)} \gamma \log(1 - p(e_{ij_k} - 1)))  }

The objective function is: \deqn{ O = \sum_{(i,j)\in E} w_{ij} (\log p(e_{ij} = 1) + \sum_{k=1}^{M} E_{jk~P_{n}(j)} \gamma \log(1 - p(e_{ij_k} - 1)))  }
}

