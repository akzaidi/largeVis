% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/projectKNNs.R
\name{projectKNNs}
\alias{projectKNNs}
\title{Project a distance matrix into a lower-dimensional space}
\usage{
projectKNNs(wij, dim, sgd.batches = nrow(N) * 10000, M = 5,
  weight.pos.samples = TRUE, gamma = 7, alpha = 2, rho = 1,
  .coords = NULL, min.rho = 0.1, verbose = TRUE)
}
\arguments{
\item{dim}{The number of dimensions for the projection space}

\item{sgd.batches}{The number of edges to process during SGD; defaults to 10000 * the number of rows in x}

\item{M}{The number of negative edges to sample for each positive edge}

\item{weight.pos.samples}{Whether to sample positive edges according to their edge weights (the default) or multiply the edge-loss by the edge-weight in the objective function.}

\item{gamma}{Hyperparameter analogous to the strength of the force operating to push-away negative examples.}

\item{alpha}{Hyperparameter used in the default distance function, \eqn{1 / (1 + \alpha \dot ||y_i - y_j||^2)}.  If \code{alpha} is 0, the alternative distance
function \eqn{1 / 1 + exp(||y_i - y_j||^2)} is used instead.  These functions relate the distance between points in the low-dimensional projection to the likelihood
that they two points are nearest neighbors.}

\item{rho}{Initial learning rate.}

\item{.coords}{An initialized coordinate matrix.}

\item{min.rho}{Final learning rate. The learning rate declines non-linearly.  \eqn{\rho_t = \rho_{t-1} - ((\rho_{t-1} - \rho_{min}) / sgd.batches)}}

\item{verbose}{Verbosity}

\item{i}{The i-vector component of a sparse triplet matrix}

\item{j}{the j-vector component of a sparse triplet matrix}

\item{x}{the x-vector component of a sparse triplet matrix. See \link{Matrix::sparseMatrix}}
}
\value{
A dense [nrow(x),dim] matrix of the coordinates projecting x into the lower-dimensional space.
}
\description{
The input is a sparse triplet matrix showing the distances between vertices which are presumably estimated k-nearest-neighbors.
The algorithm attempts to estimate a \code{dim}-dimensional embedding using stochastic gradient descent.
}
\details{
The objective function is: \deqn{ O = \sum_{(i,j)\in E} w_{ij} (\log p(e_{ij} = 1) + \sum_{k=1}^{M} E_{jk~P_{n}(j)} \gamma \log(1 - p(e_{ij_k} - 1)))  }
}

